# -*- coding: utf-8 -*-
"""IFT6135-HW2-Q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/173XeUdDzpt-C5kswSEYS_tvXSF-aSx-H
"""

import torch
import torch.nn as nn

import numpy as np
import torch.nn.functional as F
import math
import copy
import time
from torch.autograd import Variable
import matplotlib.pyplot as plt
from torch.nn import Parameter
from torch.distributions.categorical import Categorical

def clones(module, N):
    "A helper function for producing N identical layers (each with their own parameters)."
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])

class RNNCell(nn.Module):
    """
    Our implementation of an RNN cell with dropout and tanh nonlinearity
    """

    def __init__(self, input_size, hidden_size, dp_keep_prob):
        """
        input_size:   The number of expected features in the input
        hidden_size:  The number of features in the hidden state
        dp_keep_prob: The probability of *not* dropping out units in the
                      non-recurrent connections.
        """
        super(RNNCell, self).__init__()
        self.hidden_size = hidden_size
        self.zt = nn.Linear(input_size, self.hidden_size)
        self.rt = nn.Linear(self.hidden_size, self.hidden_size)

        self.dropout = nn.Dropout(1 - dp_keep_prob)
        self.tanh = nn.Tanh()

    def init_weights_uniform(self):
        """
        initializes the weights
        """
        nn.init.uniform_(self.Wx.weight.data,
                         a=-np.sqrt(1/self.hidden_size), 
                         b=np.sqrt(1/self.hidden_size))
        nn.init.uniform_(self.Wh.weight.data,
                         a=-np.sqrt(1/self.hidden_size), 
                         b=np.sqrt(1/self.hidden_size))
        nn.init.uniform_(self.Wh.bias.data,
                         a=-np.sqrt(1/self.hidden_size),
                         b=np.sqrt(1/self.hidden_size))

    def forward(self, x, h):
        """
        Arguments:
          - x: batch_size * emb_size
          - h: batch_size * hidden_size
        Returns:
          - y: batch_size * hidden_size
          - h: batch_size * hidden_size
        """
        z = self.sigmoid(self.zt)
        r = self.sigmoid(self.rt)
        ct_cap = self.tanh(nn.linear(r*self.hidden_size, self.hidden_size))
        ct = (1-z) * h + z*ct_cap
        h = ct
        y = self.dropout(h)  # apply dropout

        return y, h

class RNN(nn.Module):
    def __init__(self, emb_size, hidden_size, seq_len, batch_size, 
                 vocab_size, num_layers, dp_keep_prob):
        """
        emb_size:     The number of units in the input embeddings
        hidden_size:  The number of hidden units per layer
        seq_len:      The length of the input sequences
        vocab_size:   The number of tokens in the vocabulary (10,000 for Penn TreeBank)
        num_layers:   The depth of the stack (i.e. the number of hidden layers at
                      each time-step)
        dp_keep_prob: The probability of *not* dropping out units in the 
                      non-recurrent connections.
                      Do not apply dropout on recurrent connections.

        """
        super(RNN, self).__init__()
        # parameters
        self.emb_size = emb_size
        self.hidden_size = hidden_size
        self.seq_len = seq_len
        self.batch_size = batch_size
        self.vocab_size = vocab_size
        self.num_layers = num_layers
        self.dp_keep_prob = dp_keep_prob
        # embedding
        self.emb_layer = nn.Embedding(num_embeddings=self.vocab_size,
                                      embedding_dim=self.emb_size)
        self.emb_dropout = nn.Dropout(1 - self.dp_keep_prob)
        # list of hidden layers
        self.hidden_layers = nn.ModuleList()
        for i in range(self.num_layers):
            input_size = self.emb_size if i == 0 else self.hidden_size
            hidden_layer = RNNCell(input_size, self.hidden_size, self.dp_keep_prob)
            self.hidden_layers.append(hidden_layer)
        # output layer
        self.output_layer = nn.Linear(self.hidden_size, self.vocab_size, bias=True)
        # initialize weights
        self.init_weights_uniform()

    def init_weights(self):
        """
        Initialize the embedding and output weights uniformly in the range [-0.1, 0.1]
        and output biases to 0 (in place). The embeddings should not use a bias vector.
        Initialize all other (i.e. recurrent and linear) weights AND biases uniformly 
        in the range [-k, k] where k is the square root of 1/hidden_size
        """
        # embedding
        nn.init.uniform_(self.emb_layer.weight.data, a=-0.1, b=0.1)
        # hidden
        for hidden_layer in self.hidden_layers:
            hidden_layer.init_weights_uniform()
        # output
        nn.init.uniform_(self.output_layer.weight.data, a=-0.1, b=0.1)
        nn.init.zeros_(self.output_layer.bias.data)

    def init_hidden(self):
        """
        Initialize the hidden states to zero
        This is used for the first mini-batch in an epoch, only.
        """
        return torch.zeros(self.num_layers, self.batch_size, self.hidden_size)

    def forward(self, inputs, hidden):
        """
        Arguments:
        - inputs: A mini-batch of input sequences, composed of integers that 
                    represent the index of the current token(s) in the vocabulary.
                        shape: (seq_len, batch_size)
        - hidden: The initial hidden states for every layer of the stacked RNN.
                            shape: (num_layers, batch_size, hidden_size)

        Returns:
        - Logits for the softmax over output tokens at every time-step.
              **Do NOT apply softmax to the outputs!**
              Pytorch's CrossEntropyLoss function (applied in ptb-lm.py) does 
              this computation implicitly.
                    shape: (seq_len, batch_size, vocab_size)
        - The final hidden states for every layer of the stacked RNN.
              These will be used as the initial hidden states for all the 
              mini-batches in an epoch, except for the first, where the return 
              value of self.init_hidden will be used.
              See the repackage_hiddens function in ptb-lm.py for more details, 
              if you are curious.
                    shape: (num_layers, batch_size, hidden_size)
        """
        # outputs at each time step
        logits = torch.zeros([self.seq_len, self.batch_size, self.vocab_size],
                             device=inputs.device)
        # input embedding
        emb_inputs = self.emb_layer(inputs)
        emb_inputs = self.emb_dropout(emb_inputs)
        # loop over time steps
        for t in range(self.seq_len):
            # input at this time step
            layer_input = emb_inputs[t]
            hidden_next_list = []  # next timestep
            # loop over layers
            for i, hidden_layer in enumerate(self.hidden_layers):
                hidden_out, _ = hidden_layer(layer_input, hidden[i])  # output of hidden layer
                layer_input = hidden_out  # feed as the input for the next layer
                hidden_next_list.append(hidden_out)  # add to the list
            # stack to get the hidden layers for this timestep
            hidden = torch.stack(hidden_next_list)
            # output at this timestep
            logits[t] = self.output_layer(layer_input)
        
        return logits, hidden  # for last timestep

    def generate(self, input, hidden, generated_seq_len):
        """
        Arguments:
        - input: A mini-batch of input tokens (NOT sequences!)
                        shape: (batch_size)
        - hidden: The initial hidden states for every layer of the stacked
                  RNN.
                        shape: (num_layers, batch_size, hidden_size)
        - generated_seq_len: The length of the sequence to generate.
                       Note that this can be different than the length used 
                       for training (self.seq_len)
        Returns:
        - Sampled sequences of tokens
                    shape: (generated_seq_len, batch_size)
        """
        tokens = input.view(1, -1)  # reshape
        emb_inputs = self.emb_layer(tokens)  # embedding
        emb_inputs = self.emb_dropout(emb_input)  # dropout
        # loop over time step
        for t in range(generated_seq_len):
            # input at this time step
            layer_input = emb_inputs[0]
            hidden_next_list = []  # next timestep
            # loop over layers
            for i, hidden_layer in enumerate(self.hidden_layers):
                hidden_out, _ = hidden_layer(layer_input, hidden[i])  # output of hidden layer
                layer_input = hidden_out  # feed as the input for the next layer
                hidden_next_list.append(hidden_out)  # add to the list
            # stack to get the hidden layers for this timestep
            hidden = torch.stack(hidden_next_list)
            # output at this timestep
            logits = self.output_layer(layer_input).detach()
            probs = F.softmax(logits, dim=1)
            # append output
            samples = Categorical(probs=probs).sample().view(1, -1)
            tokens = torch.cat((tokens, samples), dim=0)
            # embed to get next input
            emb_inputs = self.emb_layer(tokens)  # embedding
            emb_inputs = self.emb_dropout(emb_input)  # dropout

        return tokens

nn.Embedding?

