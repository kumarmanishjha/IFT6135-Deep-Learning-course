{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IFT6135_Assignment_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3glecRTQK9Q"
      },
      "source": [
        "# IFT 6135 - Assignment 1\n",
        "## Authors: Manish Jha, Shivendra Bhardwaj, Tapopriya Majumdar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vCSbDrmjv0j"
      },
      "source": [
        "# the code is also available at https://github.com/kumarmanishjha/6135_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fZE8FyYPzRV"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gzip\n",
        "import pickle\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ-rmUYBQcLU"
      },
      "source": [
        "## Question 1\n",
        "\n",
        "Here we implement our version of the MLP. with multiple hidden layers as a class ``NN``. For the MNIST dataset, we choose a network with two hidden layers: $h^1=512$ and $h^2=1024$, so that the total number of parameters is $937482$. The method ``initialize_params`` initializes the weight and bias parameters according to the three options: ``zero``, ``normal`` or ``glorot``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcJ9UxzjPzRk"
      },
      "source": [
        "class NN:\n",
        "    \"\"\"Our implementation of a Neural Network\"\"\"\n",
        "    \n",
        "    def __init__(self, hidden_dims=(1024, 2048), n_hidden=2,\n",
        "                 mode='train', datapath=None, model_path=None):\n",
        "        \"\"\"\n",
        "        Initialize an MLP instance\n",
        "        \"\"\"\n",
        "        # input check\n",
        "        assert len(hidden_dims) == n_hidden, \"Check input data!\"\n",
        "        assert mode in ['train', 'test'], \"Check input data!\"\n",
        "        for d in hidden_dims:\n",
        "            assert isinstance(d, int) & (d > 0), \"Check input data!\"\n",
        "        self.n_hidden = n_hidden\n",
        "        if mode == 'train':\n",
        "            # load the pickled data\n",
        "            with gzip.open(datapath, 'rb') as f:\n",
        "                data = pickle.load(f, encoding='latin-1')\n",
        "            # construct the training, validation \n",
        "            # and test datasets\n",
        "            (self.X_train, self.y_train), \\\n",
        "                (self.X_valid, self.y_valid), \\\n",
        "                (self.X_test, self.y_test) = data\n",
        "\n",
        "            # store the dimensions\n",
        "            self.dims = [self.X_train.shape[1]] + list(hidden_dims) \\\n",
        "                                                + [len(set(self.y_train))]\n",
        "            self.n_classes = self.dims[-1]\n",
        "\n",
        "    def initialize_weights(self, init_method='glorot', seed=False):\n",
        "        \"\"\"\n",
        "        Intialize the weights and biases\n",
        "        according to the initialization method\n",
        "        \"\"\"\n",
        "        assert init_method in ['zero', 'normal', 'glorot'], \\\n",
        "                                                \"Check input data!\"\n",
        "        params = {}  #  dictionary to store weights and biases\n",
        "        n_params = 0  # running counter for parameters\n",
        "        if seed:  # for reproducibility\n",
        "            np.random.seed(seed)\n",
        "        if init_method == 'zero':\n",
        "            for i in range(self.n_hidden+1):\n",
        "                # zero matrices and vectors\n",
        "                W = np.zeros((self.dims[i+1], self.dims[i]))\n",
        "                b = np.zeros((self.dims[i+1], 1))\n",
        "                n_params += W.size + b.size\n",
        "                # params update\n",
        "                params.update({\"W\"+str(i+1): W, \"b\"+str(i+1): b})\n",
        "\n",
        "        elif init_method == 'normal':\n",
        "            for i in range(self.n_hidden+1):\n",
        "                # sample from normal\n",
        "                #W = np.random.normal(size=(self.dims[i+1], self.dims[i]))\n",
        "                W = np.random.randn(self.dims[i+1], self.dims[i])\n",
        "                b = np.zeros((self.dims[i+1], 1))  # zero vector\n",
        "                n_params += W.size + b.size\n",
        "                # params update\n",
        "                params.update({\"W\"+str(i+1): W, \"b\"+str(i+1): b})\n",
        "\n",
        "        elif init_method == 'glorot':\n",
        "            for i in range(self.n_hidden+1):\n",
        "                # sample from uniform [-di, di]\n",
        "                di = np.sqrt(6 /(self.dims[i] + self.dims[i+1]))\n",
        "                W = np.random.uniform(low=-di, high=di,\n",
        "                                      size=(self.dims[i+1], self.dims[i]))\n",
        "                b = np.zeros((self.dims[i+1], 1))  # zero vector\n",
        "                n_params += W.size + b.size\n",
        "                # params update\n",
        "                params.update({\"W\"+str(i+1): W, \"b\"+str(i+1): b})\n",
        "        \n",
        "        # store number of parameters as well\n",
        "        params.update({\"n_params\": n_params})\n",
        "        return params\n",
        "\n",
        "    def activation(self, z, function=\"relu\"):\n",
        "        \"\"\"Computing different activation functions\"\"\"\n",
        "        assert function in [\"relu\", \"sigmoid\", \"tanh\"], \\\n",
        "                                \"Check your activation function!\"\n",
        "            \n",
        "        if function == \"relu\":\n",
        "            return np.maximum(z, 0)\n",
        "        \n",
        "        elif function == \"sigmoid\":\n",
        "            # numerically stable\n",
        "            return np.where(z >= 0, \n",
        "                            1 / (1 + np.exp(-z)), \n",
        "                            np.exp(z) / (1 + np.exp(z)))\n",
        "        \n",
        "        elif function == \"tanh\":\n",
        "            #numerically stable\n",
        "            return np.where(z >= 0, \n",
        "                            (1 - np.exp(-2*z)) / (1 + np.exp(-2*z)), \n",
        "                            (np.exp(2*z) - 1) / (np.exp(2*z) + 1))\n",
        "\n",
        "    def softmax(self, X):\n",
        "        \"\"\"Numerically stable softmax\"\"\"\n",
        "        X_max = np.max(X, axis=0, keepdims=True)\n",
        "        exp_X = np.exp(X - X_max)\n",
        "        return exp_X / np.sum(exp_X, axis=0, keepdims=True)\n",
        "\n",
        "    def loss(self, y, y_hat):\n",
        "        \"\"\"Compute the cross-entropy loss for multiple entries\"\"\"\n",
        "        #print(min(y_hat[y, range(len(y))]), max(y_hat[y, range(len(y))]))\n",
        "        probs = y_hat[y, range(len(y))]\n",
        "        probs[probs == 0] = 1e-3\n",
        "        return -np.log(probs)\n",
        "\n",
        "    def forward(self, X, params, activation_function='relu'):\n",
        "        \"\"\"Forward propagation\"\"\"\n",
        "        cache = {}  # dictionary to store the outputs\n",
        "        # start with the data (transpose for convenience)\n",
        "        Z = X.T\n",
        "        for i in range(self.n_hidden+1):  # hidden layers\n",
        "            # retrieve the relevant parameters\n",
        "            W = params[\"W\"+str(i+1)]\n",
        "            b = params[\"b\"+str(i+1)]\n",
        "            # affine transformation & nonlinearity\n",
        "            A = np.dot(W, Z) + b  \n",
        "            if i != self.n_hidden:\n",
        "                Z = self.activation(A, function=activation_function)\n",
        "            else:\n",
        "                Z = self.softmax(A)\n",
        "            # update the cache\n",
        "            cache.update({\"A\"+str(i+1): A, \"Z\"+str(i+1): Z})\n",
        "        return cache\n",
        "\n",
        "    def backward(self, X, y, params, cache, activation_function=\"relu\"):\n",
        "        \"\"\"Backward Propagation\"\"\"\n",
        "        grads = {}  # store gradients\n",
        "        # gradient wrt the softmax function\n",
        "        eye_mat = np.zeros((self.n_classes, len(y)))\n",
        "        for j in range(len(y)):\n",
        "            eye_mat[y[j], j] = 1\n",
        "        grad_A = cache[\"Z\"+str(self.n_hidden+1)] - eye_mat\n",
        "        # update parameter gradients\n",
        "        for i in range(self.n_hidden+1, 0, -1):\n",
        "            # define Z using cases\n",
        "            Z = cache[\"Z\"+str(i-1)] if i != 1 else X.T \n",
        "            # compute gradients\n",
        "            grad_W = np.dot(grad_A, Z.T)\n",
        "            grad_b = np.sum(grad_A, axis=1, keepdims=True)\n",
        "            # put updated grads in the dictionary\n",
        "            grads.update({\"grad_W\"+str(i): grad_W, \n",
        "                          \"grad_b\"+str(i): grad_b})\n",
        "            # no need to calculate grad_A0\n",
        "            if i == 1:\n",
        "                continue\n",
        "            # derivative of the activation functions\n",
        "            A = cache[\"A\"+str(i-1)]\n",
        "            activation_derivative = \\\n",
        "                (A > 0) * 1 if activation_function == \"relu\" else \\\n",
        "                A * (1 - A) if activation_function == \"sigmoid\" else \\\n",
        "                1 - (A ** 2)  # tanh\n",
        "            # grad_A for the previous layer\n",
        "            grad_Z = np.dot(params[\"W\"+str(i)].T, grad_A)\n",
        "            grad_A =  grad_Z * activation_derivative\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def update(self, y, grads, params, learning_rate=0.01):\n",
        "        \"\"\"Update the weights\"\"\"\n",
        "        # loop over the layers\n",
        "        batch_size = len(y)\n",
        "        #print('batch_size:', batch_size)\n",
        "        for i in range(1, self.n_hidden+2):\n",
        "            # get the parameters & their gradients\n",
        "            W = params[\"W\"+str(i)]\n",
        "            b = params[\"b\"+str(i)]\n",
        "            grad_W = grads[\"grad_W\"+str(i)]\n",
        "            grad_b = grads[\"grad_b\"+str(i)]\n",
        "            # update using gradient descent\n",
        "            W -= (learning_rate / batch_size) * grad_W\n",
        "            b -= (learning_rate / batch_size) * grad_b\n",
        "            # updated the parameters\n",
        "            params.update({\"W\"+str(i): W, \"b\"+str(i): b})\n",
        "\n",
        "        return params\n",
        "\n",
        "    def train(self, X, y, init_method='glorot', seed=False, learning_rate=0.01,\n",
        "              n_epochs=10, batch_size=200, activation_function=\"relu\", verbose=True):\n",
        "        \"\"\"Train the neural network\"\"\"\n",
        "        # encode the labels\n",
        "        #y = np.eye(self.n_classes)[y]\n",
        "        # initialize parms\n",
        "        params = self.initialize_weights(init_method)\n",
        "        # lists to store values for each epoch\n",
        "        avg_losses, train_accs, val_accs = [], [], []\n",
        "        # number of minibatches\n",
        "        n_minibatches = 1 + (len(y) // batch_size)\n",
        "        l = 1\n",
        "        while True:\n",
        "            # check number of epochs\n",
        "            if l > n_epochs:\n",
        "                if verbose:\n",
        "                    print(\"Maximum epochs reached. Exiting...\")\n",
        "                break\n",
        "            # loop over minibatches\n",
        "            losses = []  # losses for the current epoch\n",
        "            for i in range(n_minibatches):\n",
        "                # final batch, if exists\n",
        "                if i*batch_size >= len(y):\n",
        "                    continue\n",
        "                # set the current batch\n",
        "                X_batch = X[i*batch_size:(i+1)*batch_size]\n",
        "                y_batch = y[i*batch_size:(i+1)*batch_size]\n",
        "                # do fprop, bprop and update params\n",
        "                cache = self.forward(X_batch, params, activation_function)\n",
        "                grads = self.backward(X_batch, y_batch, params, cache, activation_function)\n",
        "                params = self.update(y_batch, grads, params, learning_rate)\n",
        "                # add the losses\n",
        "                losses.extend(self.loss(y_batch, cache[\"Z\"+str(self.n_hidden+1)]))\n",
        "            # find average loss over the epoch\n",
        "            avg_losses.append(np.mean(losses))\n",
        "            # training accuracy\n",
        "            train_acc = self.test(self.X_train, self.y_train, params, activation_function)\n",
        "            train_accs.append(train_acc)\n",
        "            # validation accuracy\n",
        "            val_acc = self.test(self.X_valid, self.y_valid, params, activation_function)\n",
        "            val_accs.append(val_acc)\n",
        "            if verbose:\n",
        "                print(\"Epoch %d/%d - loss: %0.4f - acc: %0.4f - val_acc: %0.4f\"\n",
        "                        % (l, n_epochs, avg_losses[-1], train_acc, val_acc))\n",
        "            l += 1 # increase counter\n",
        "\n",
        "        return avg_losses, train_accs, val_accs\n",
        "\n",
        "    def test(self, X, y, params, activation_function='relu'):\n",
        "        \"\"\"Test accuracy\"\"\"\n",
        "        cache = self.forward(X, params, activation_function)\n",
        "        pred = np.argmax(cache[\"Z\"+str(self.n_hidden+1)], axis=0)\n",
        "        \n",
        "        return np.sum(y == pred) / len(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO8CFGOoSxn5"
      },
      "source": [
        "### Check the total number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiQy2M35RBcG",
        "outputId": "c0550dc9-a1b5-437f-cead-0959adc9e528",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "nn = NN(datapath='mnist.pkl.gz', hidden_dims=(512, 1024))\n",
        "params = nn.initialize_weights()\n",
        "params['n_params']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "937482"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFOOolJWTzsh"
      },
      "source": [
        "### Different Initialization Techniques\n",
        "\n",
        "We compare results after training for $10$ epochs with different initialization methods. We notice that the ``zero`` method does not optimize at all, and ``glorot`` converges faster than ``normal``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsdckSADwJFa",
        "outputId": "2b3b396e-281b-44a1-f6cd-689095949945",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "nn = NN(datapath='mnist.pkl.gz', hidden_dims=(512, 768))\n",
        "glorot_losses, _, _ = nn.train(nn.X_train, nn.y_train, learning_rate=0.01, init_method='glorot', batch_size=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10 - loss: 2.1815 - acc: 0.5717 - val_acc: 0.5890\n",
            "Epoch 2/10 - loss: 1.8652 - acc: 0.7138 - val_acc: 0.7308\n",
            "Epoch 3/10 - loss: 1.5412 - acc: 0.7609 - val_acc: 0.7808\n",
            "Epoch 4/10 - loss: 1.2429 - acc: 0.7943 - val_acc: 0.8167\n",
            "Epoch 5/10 - loss: 1.0160 - acc: 0.8181 - val_acc: 0.8408\n",
            "Epoch 6/10 - loss: 0.8589 - acc: 0.8343 - val_acc: 0.8554\n",
            "Epoch 7/10 - loss: 0.7504 - acc: 0.8459 - val_acc: 0.8640\n",
            "Epoch 8/10 - loss: 0.6730 - acc: 0.8541 - val_acc: 0.8724\n",
            "Epoch 9/10 - loss: 0.6156 - acc: 0.8612 - val_acc: 0.8796\n",
            "Epoch 10/10 - loss: 0.5714 - acc: 0.8667 - val_acc: 0.8849\n",
            "Maximum epochs reached. Exiting...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4132Dls_i5a",
        "outputId": "1f10d58f-e283-4856-9307-3879be611c7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "normal_losses, _, _ = nn.train(nn.X_train, nn.y_train, learning_rate=0.01, init_method='normal', batch_size=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10 - loss: 36.7634 - acc: 0.8603 - val_acc: 0.8652\n",
            "Epoch 2/10 - loss: 25.4217 - acc: 0.8925 - val_acc: 0.8910\n",
            "Epoch 3/10 - loss: 21.5557 - acc: 0.9071 - val_acc: 0.9011\n",
            "Epoch 4/10 - loss: 18.6824 - acc: 0.9195 - val_acc: 0.9081\n",
            "Epoch 5/10 - loss: 16.8727 - acc: 0.9261 - val_acc: 0.9107\n",
            "Epoch 6/10 - loss: 15.2261 - acc: 0.9306 - val_acc: 0.9128\n",
            "Epoch 7/10 - loss: 13.6288 - acc: 0.9366 - val_acc: 0.9141\n",
            "Epoch 8/10 - loss: 12.4310 - acc: 0.9411 - val_acc: 0.9155\n",
            "Epoch 9/10 - loss: 11.3152 - acc: 0.9456 - val_acc: 0.9179\n",
            "Epoch 10/10 - loss: 10.1651 - acc: 0.9481 - val_acc: 0.9196\n",
            "Maximum epochs reached. Exiting...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNe9CQOiww04",
        "outputId": "ad35d612-b9e9-40c1-d0d0-f8484eea8fff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "zero_losses, _, _ = nn.train(nn.X_train, nn.y_train, learning_rate=0.1, init_method='zero', batch_size=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10 - loss: 2.3020 - acc: 0.1136 - val_acc: 0.1064\n",
            "Epoch 2/10 - loss: 2.3014 - acc: 0.1136 - val_acc: 0.1064\n",
            "Epoch 3/10 - loss: 2.3012 - acc: 0.1136 - val_acc: 0.1064\n",
            "Epoch 4/10 - loss: 2.3011 - acc: 0.1136 - val_acc: 0.1064\n",
            "Epoch 5/10 - loss: 2.3011 - acc: 0.1136 - val_acc: 0.1064\n",
            "Epoch 6/10 - loss: 2.3010 - acc: 0.1136 - val_acc: 0.1064\n",
            "Epoch 7/10 - loss: 2.3010 - acc: 0.1136 - val_acc: 0.1064\n",
            "Epoch 8/10 - loss: 2.3010 - acc: 0.1136 - val_acc: 0.1064\n",
            "Epoch 9/10 - loss: 2.3010 - acc: 0.1136 - val_acc: 0.1064\n",
            "Epoch 10/10 - loss: 2.3010 - acc: 0.1136 - val_acc: 0.1064\n",
            "Maximum epochs reached. Exiting...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIb4tS-h6jQC",
        "outputId": "4f775c0f-8d27-4fe7-9bc8-c3f7c2e0942e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        }
      },
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "plt.title('Initialization Method Comparison')\n",
        "plt.plot(range(1, 11), zero_losses, label='zero')\n",
        "plt.plot(range(1, 11), normal_losses, label='normal')\n",
        "plt.plot(range(1, 11), glorot_losses, label='glorot')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr4AAAHhCAYAAACIt73KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/D/RwIyhA0EgGAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQFdp4IAS7iA"
      },
      "source": [
        "### Hyperparameter search with ``glorot`` initialization\n",
        "\n",
        "We look at a few hyperparaneter combinations, and note that ``relu`` activation with ``learning_rate=0.1`` and ``batch_size=200`` achieves more than $97\\%$ accuracy on the validation set. We also try some other combinations and see that they all achieve more than $90\\%$ validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXx200H5PzRz",
        "outputId": "86ef1a90-12c6-4e4b-801a-f6c5862f304e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "nn = NN(datapath='mnist.pkl.gz', hidden_dims=(512, 1024))\n",
        "#params = nn.initialize_weights()\n",
        "#params['n_params']\n",
        "out = nn.train(nn.X_train, nn.y_train, learning_rate=0.1, activation_function='relu', batch_size=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10 - loss: 0.5384 - acc: 0.8999 - val_acc: 0.9128\n",
            "Epoch 2/10 - loss: 0.2626 - acc: 0.9296 - val_acc: 0.9352\n",
            "Epoch 3/10 - loss: 0.2066 - acc: 0.9453 - val_acc: 0.9488\n",
            "Epoch 4/10 - loss: 0.1709 - acc: 0.9553 - val_acc: 0.9566\n",
            "Epoch 5/10 - loss: 0.1453 - acc: 0.9622 - val_acc: 0.9612\n",
            "Epoch 6/10 - loss: 0.1258 - acc: 0.9681 - val_acc: 0.9648\n",
            "Epoch 7/10 - loss: 0.1103 - acc: 0.9718 - val_acc: 0.9683\n",
            "Epoch 8/10 - loss: 0.0976 - acc: 0.9753 - val_acc: 0.9703\n",
            "Epoch 9/10 - loss: 0.0870 - acc: 0.9779 - val_acc: 0.9708\n",
            "Epoch 10/10 - loss: 0.0779 - acc: 0.9804 - val_acc: 0.9719\n",
            "Maximum epochs reached. Exiting...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhKrIP_LKt6p",
        "outputId": "176eb06d-11c2-45ae-c36d-6cc4e05c98ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "out = nn.train(nn.X_train, nn.y_train, learning_rate=0.05, activation_function='relu', batch_size=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10 - loss: 0.7128 - acc: 0.8926 - val_acc: 0.9049\n",
            "Epoch 2/10 - loss: 0.3247 - acc: 0.9151 - val_acc: 0.9239\n",
            "Epoch 3/10 - loss: 0.2709 - acc: 0.9288 - val_acc: 0.9337\n",
            "Epoch 4/10 - loss: 0.2370 - acc: 0.9375 - val_acc: 0.9429\n",
            "Epoch 5/10 - loss: 0.2113 - acc: 0.9443 - val_acc: 0.9486\n",
            "Epoch 6/10 - loss: 0.1904 - acc: 0.9492 - val_acc: 0.9529\n",
            "Epoch 7/10 - loss: 0.1729 - acc: 0.9535 - val_acc: 0.9559\n",
            "Epoch 8/10 - loss: 0.1581 - acc: 0.9572 - val_acc: 0.9589\n",
            "Epoch 9/10 - loss: 0.1453 - acc: 0.9604 - val_acc: 0.9610\n",
            "Epoch 10/10 - loss: 0.1342 - acc: 0.9634 - val_acc: 0.9637\n",
            "Maximum epochs reached. Exiting...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Tm_ox3KLrDu",
        "outputId": "505ff082-e506-443b-ebc7-bcf4beee9ceb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "out = nn.train(nn.X_train, nn.y_train, learning_rate=0.01, activation_function='tanh', batch_size=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10 - loss: 1.0775 - acc: 0.8489 - val_acc: 0.8684\n",
            "Epoch 2/10 - loss: 0.5411 - acc: 0.8750 - val_acc: 0.8885\n",
            "Epoch 3/10 - loss: 0.4447 - acc: 0.8862 - val_acc: 0.8995\n",
            "Epoch 4/10 - loss: 0.4008 - acc: 0.8933 - val_acc: 0.9053\n",
            "Epoch 5/10 - loss: 0.3745 - acc: 0.8987 - val_acc: 0.9086\n",
            "Epoch 6/10 - loss: 0.3564 - acc: 0.9021 - val_acc: 0.9129\n",
            "Epoch 7/10 - loss: 0.3428 - acc: 0.9050 - val_acc: 0.9144\n",
            "Epoch 8/10 - loss: 0.3320 - acc: 0.9077 - val_acc: 0.9167\n",
            "Epoch 9/10 - loss: 0.3230 - acc: 0.9097 - val_acc: 0.9178\n",
            "Epoch 10/10 - loss: 0.3152 - acc: 0.9115 - val_acc: 0.9189\n",
            "Maximum epochs reached. Exiting...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylJlS_g_OgF2",
        "outputId": "f6badd79-78e0-4d85-983a-5ce33296ce14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "out = nn.train(nn.X_train, nn.y_train, learning_rate=0.1, activation_function='relu', batch_size=400)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10 - loss: 0.7303 - acc: 0.8899 - val_acc: 0.9047\n",
            "Epoch 2/10 - loss: 0.3289 - acc: 0.9143 - val_acc: 0.9233\n",
            "Epoch 3/10 - loss: 0.2717 - acc: 0.9296 - val_acc: 0.9356\n",
            "Epoch 4/10 - loss: 0.2360 - acc: 0.9390 - val_acc: 0.9441\n",
            "Epoch 5/10 - loss: 0.2092 - acc: 0.9460 - val_acc: 0.9500\n",
            "Epoch 6/10 - loss: 0.1879 - acc: 0.9510 - val_acc: 0.9540\n",
            "Epoch 7/10 - loss: 0.1703 - acc: 0.9550 - val_acc: 0.9574\n",
            "Epoch 8/10 - loss: 0.1555 - acc: 0.9590 - val_acc: 0.9599\n",
            "Epoch 9/10 - loss: 0.1428 - acc: 0.9625 - val_acc: 0.9631\n",
            "Epoch 10/10 - loss: 0.1319 - acc: 0.9653 - val_acc: 0.9650\n",
            "Maximum epochs reached. Exiting...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aci12wJKdCEm",
        "outputId": "84303853-0e34-4a3b-e156-2e0b04d5f25c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "out = nn.train(nn.X_train, nn.y_train, learning_rate=0.01, activation_function='tanh', batch_size=400)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10 - loss: 1.3997 - acc: 0.8100 - val_acc: 0.8367\n",
            "Epoch 2/10 - loss: 0.7443 - acc: 0.8484 - val_acc: 0.8697\n",
            "Epoch 3/10 - loss: 0.5800 - acc: 0.8649 - val_acc: 0.8816\n",
            "Epoch 4/10 - loss: 0.5045 - acc: 0.8741 - val_acc: 0.8884\n",
            "Epoch 5/10 - loss: 0.4602 - acc: 0.8803 - val_acc: 0.8945\n",
            "Epoch 6/10 - loss: 0.4307 - acc: 0.8857 - val_acc: 0.8989\n",
            "Epoch 7/10 - loss: 0.4094 - acc: 0.8896 - val_acc: 0.9006\n",
            "Epoch 8/10 - loss: 0.3931 - acc: 0.8929 - val_acc: 0.9016\n",
            "Epoch 9/10 - loss: 0.3801 - acc: 0.8956 - val_acc: 0.9045\n",
            "Epoch 10/10 - loss: 0.3695 - acc: 0.8979 - val_acc: 0.9056\n",
            "Maximum epochs reached. Exiting...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du-VWHYtXTsx"
      },
      "source": [
        "### Validate gradients with finite gradient check\n",
        "\n",
        "We compute finite difference gradients and check how well they approximate the gradients for a random data point. From the plot, we check see that our finite difference gradients are pretty good approximations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spvi8Bl4ZsLv",
        "outputId": "79f8dcce-5486-4399-cc19-5e1151b4c8dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.random.choice([k*10**i for i in range(6) for k in [1,5]],\n",
        "                      5, replace=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  1000, 500000,     50,  50000,  10000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Og9qD-H-TVvz"
      },
      "source": [
        "# load the model\n",
        "nn = NN(datapath='mnist.pkl.gz', hidden_dims=(512, 768))\n",
        "# values of N\n",
        "Ns = sorted(np.random.choice([k*10**i for i in range(6) for k in [1,5]], 5, replace=False))\n",
        "# obtain one random datapoint\n",
        "np.random.seed(0)\n",
        "i = np.random.randint(0, nn.X_train.shape[0])\n",
        "X, y = nn.X_train[i:(i+1)], nn.y_train[i:(i+1)]\n",
        "# compute forward and backward prop and the gradient\n",
        "params = nn.initialize_weights()\n",
        "cache = nn.forward(X, params)\n",
        "grads = nn.backward(X, y, params, cache)\n",
        "# take the first 10 parameters \n",
        "grad_theta = grads['grad_W2'][:10, 0]\n",
        "differences = []\n",
        "for N in Ns:\n",
        "    eps = 1 / N\n",
        "    grad_diff = np.zeros(10)\n",
        "    for i in range(10):\n",
        "        # compute finite difference\n",
        "        params['W2'][i, 0] += eps\n",
        "        cache = nn.forward(X, params)\n",
        "        # add epsilon \n",
        "        l_pos = nn.loss(y, cache[\"Z\"+str(nn.n_hidden+1)])\n",
        "        # subtract epsilon\n",
        "        params['W2'][i, 0] -= 2 * eps\n",
        "        cache = nn.forward(X, params)\n",
        "        l_neg = nn.loss(y, cache[\"Z\"+str(nn.n_hidden+1)])\n",
        "        # reset value\n",
        "        params['W2'][i, 0] += eps\n",
        "        # Finite Diff\n",
        "        grad_diff[i] = (l_pos - l_neg) / (2 * eps)\n",
        "    differences.append(np.max(np.abs(grad_theta - grad_diff)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_W6s0gnfgp-",
        "outputId": "06e2a094-7df8-4d31-d17c-0d9e125f24b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "Ns, differences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([10, 50, 5000, 50000, 500000],\n",
              " [0.01071417955257601,\n",
              "  1.252289438813392e-09,\n",
              "  9.091642369507547e-13,\n",
              "  1.473225361148245e-11,\n",
              "  1.1899492242256615e-10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkayFCOhfviZ",
        "outputId": "eb94b956-2c16-43fe-fcda-46cd327c60db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "plt.plot(range(5), differences, 'k.', markersize=20)\n",
        "plt.xlabel('N')\n",
        "plt.ylabel('Difference')\n",
        "plt.xticks([])\n",
        "plt.title('Gradient Difference')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Gradient Difference')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEECAYAAADZBhiGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAFutJREFUeJzt3X+U3XV95/HnkLHAxAyMMBKgdCOI\n7wgWF/yxsCxEFxZ/kaKF3bULVoSiAm1TWHEPXdHFSFD5MUuoawsWqtjjoZUGSJtaanuw1dizNHYL\nu8V3QApEAmQ4iUkgFvJj9o/vd+A6mXs/9yZzZyaZ5+OcOTP3+/l8vt/3fMW85vv5fL/39oyMjCBJ\nUiv7THUBkqTpz7CQJBUZFpKkIsNCklRkWEiSigwLSVJR71QXILUjInqA3wAuBH4OeBWwGrgqM1dN\nwP63Aa8HjgcWZuYFu7GvizLz1nG2nw98CVhDVT/AvcDVmfmTus8PgQXAc8BfAq8Dfgk4Bfg0cHNm\nXrOrtUm7yisL7SmuAX4FeHdmvhEI4B7g2xExOFEHycxluxkUc4FPtujy/cycn5lHAScA+wH3R8R+\n9fHnZ+azwGFUofGGzHwIOBv47waFpkqPD+VpuouI1wA/Bt6cmY+MaZuTmZvrn+8Hvgf8MtUVyI+A\nrwLzgH2p/iq/se77HuBmYCtwG3At1ZXFO4DzMvP0iDiw7vNvqK7CF2fm7fX4EeBXgcuBucAXM3Mo\nIh4Dfh54DDguM19qqPX80X2P+R3+FrgjM2+p9zsP+GvgSCCBh4F3AT8BbqlrvQ54N9VV1i2ZuaTe\n1+P173Mu8B+AHcCXqcIVYFFm/nlEzAO+X+/rIuA1wOWZeWd9FXcD8IH6/NyamdfV26+q970fcHc9\nZvt4/7tp7+KVhfYEJwJPjg0KgNGgaPAW4NjMXAl8CvjnzJwPnAZcGxFHRMQs4PeBS+qrlB3ArHGO\ne0PdNp8qMK6OiDc1tB+bmcdTTRMtqfd7QV3r/MagKFgOvLPh9fa63u31fj4A/G/gk5n5P6iuXI4B\nfhE4FjgnIs5sGP/zmRmZ+SRVWP6fzHwD8F7g6xFxUN3vYGBHZv4i8FvA5+rt5wJvB94AvBX4jYh4\nO3Ae8J/qtqPqr4vb/B21hzMstCcYAIZHX0TEgRHxw/rrxxHROO2zIjN31D//JtU6B5n5GPAM1RrA\n0cB+mXlf3e8Pmhx3IXBTZu7IzGHgT6iuWkbdUX//AdVf2q/dxd9vE3BAB/0XAv8rM1/MzBeAr42p\n608BImI2VQgNAWTmo8DfAu+r+/UCtzf8Dr9Q//xe4JuZuTUzNwFvBB6oj3tbZm7MzG3AV8YcV3sx\nF7i1JximmsMHoF4Mng8QEV8B+hr6rm/4+W1UVxO/QPXX+qFUfyC9huof6FEbmhz3QOCP6sVvgP2B\nP25o31jXsz0iYPyrk3bMA9Z10P9AYCgiltSv96W68hg1eg4OAHqAlXV9AK+mmuKC6srlhdGfeaX+\ng6mmvAAY7VNPy30iIj5aN/XSEOLauxkW2hN8H3htRByfmf/QwbivU/1V/buZORIRT9XbNwD9Df2a\nLZCvBd6fmf+344rbVE9dvR/4bAfD1gLXZ+afFvqtowqBt2bm82OOO6/FuOeoAmO07yHAT+vj3puZ\nv9NBrdpLOA2laa9el1gM3BERrweIiH0i4oNUc+iPNhn6WmBVHRQfBmZT/WX9KLAtIt5R9/sIMN6d\nHvcAH6+P1xsRQxFxQqHcrcCrI6L4h1g9TXQLVXj9Uan/mLp+LSJmRURPRHwqIt49tlM9VfRnDb9D\nX0TcFhFHFPZ/L/ArEbFvXeN3gTfVx/1QRPTV+/tYfV41AxgW2iNk5heB/wl8s34W4TGqf+TPycyv\nNxl2FbAsIh6kConfA26lmpv/KHBbRDxMtYj9fJPxB0REAv+PaprmwUKpD1JNAz1TT3+NdVK91vII\n1Z1O/wK8q/6HvV1fAp6oa/oh1ZrCd5v0vRhYUJ+zHwCPZeaawv7vBP4CeAT4B+D36xsG7qZajP9B\nvb9fqvtpBvDWWUlSkVcWkqQiw0KSVGRYSJKKDAtJUtFe+ZzF8PBmV+0lqUODg3N6mrV5ZSFJKjIs\nJElFhoUkqciwkCQV7ZUL3Ltq06aNLF9+D88++wyHHDKXhQvPor+/k3eOlqS90175dh+7cjfU0NB1\n3HTTjWzZ8sLL2/r6ZrNo0eVcdtkVE1qfJE1Hre6G8sqCKiiuvXbxTtu3bHnh5e0GhqSZbMZfWWza\ntJHjjpv/M1cUY/X1zeahh5I5c/qb9pGkPZ3PWbSwfPk9LYMCqiuM5cvvmaSKJGn6mfFh8eyzz0xo\nP0naG834sDjkkLkT2k+S9kauWbhmIUmAaxYt9fcfwKJFl7fss2jR5QaFpBnNW2d55bZYn7OQpPHN\n+GmoRps3b9rpCW6vKCTNFK2moQwLSRLgmoUkaTcZFpKkIsNCklTU1buhImIIOBEYARZl5gMNbacD\nS4DtwIrMXFxvfxNwDzCUmb9TbzsCuAOYBTwNfCgzX+xm7ZKkV3TtyiIiFgBHZ+ZJwIXA0jFdlgJn\nAycDZ0TEMRExG7gZ+KsxfT8LfCkzTwEeBS7oVt2SpJ11cxrqNOBugMx8GBiIiH6AiDgSWJ+ZazJz\nB7Ci7v8i8F5g7Zh9vQO4t/55OXB6F+uWJI3RzWmoucCqhtfD9bZN9ffhhrZ1wFGZuQ3YFhFj9zW7\nYdppHXBoqwMPDPTR2ztrN0qXJDWazCe4m96/W2jruO+GDVs62J0kCWBwcE7Ttm5OQ62luoIYdRjV\n4vR4bYez89RTo+cjYv82+0qSJlg3w+I+4ByAiDgBWJuZmwEy83GgPyLmRUQvcGbdv5lvUy2GU3//\nVreKliTtrKtv9xERnwdOBXYAlwLHAxszc1lEnAp8oe56V2ZeHxFvAW4A5gFbgaeAXwb2Bb4G7Ac8\nAXwkM7c2O65v9yFJnfO9oSRJRb43lCRptxgWkqQiw0KSVGRYSJKKDAtJUpFhIUkqMiwkSUWGhSSp\nyLCQJBUZFpKkIsNCklRkWEiSigwLSVKRYSFJKjIsJElFhoUkqciwkCQVGRaSpCLDQpJUZFhIkooM\nC0lSkWEhSSoyLCRJRYaFJKnIsJAkFRkWkqQiw0KSVGRYSJKKeru584gYAk4ERoBFmflAQ9vpwBJg\nO7AiMxc3GxMRp9Z9twIvAB/KzA3drF2S9IquXVlExALg6Mw8CbgQWDqmy1LgbOBk4IyIOKbFmBuB\nCzPzncBK4GPdqluStLNuTkOdBtwNkJkPAwMR0Q8QEUcC6zNzTWbuAFbU/ZuNeQ44qN7vQP1akjRJ\nujkNNRdY1fB6uN62qf4+3NC2DjgKOLjJmMuA70TEBmADcGWrAw8M9NHbO2t365ck1bq6ZjFGzy60\njW6/GfhAZn4vIq4HLmHnaa2XbdiwZdcqlKQZbHBwTtO2bk5DraW6Khh1GPB0k7bD623NxhyXmd+r\nt/0l8NZuFCxJGl83w+I+4ByAiDgBWJuZmwEy83GgPyLmRUQvcGbdv9mYZyLimHq/bwMe6WLdkqQx\nujYNlZkrI2JVRKwEdgCXRsT5wMbMXAZcDHyj7n5nZq4GVo8dU7d/HLg1IrYC64ELulW3JGlnPSMj\nI1Ndw4QbHt689/1SktRlg4Nzmq4t+wS3JKnIsJAkFRkWkqQiw0KSVGRYSJKKDAtJUpFhIUkqMiwk\nSUWGhSSpyLCQJBUZFpKkIsNCklRkWEiSigwLSVKRYSFJKjIsJElFhoUkqciwkCQVGRaSpCLDQpJU\nZFhIkop62+kUEf8KuAE4KDPfGREXAfdn5iNdrU6SNC20e2VxK/C1hv4J3NKViiRJ0067YfGqzLwX\n2AGQmX/TvZIkSdNN22sWEXEgMFL/fCywf7eKkiRNL22tWQCfBf4OODQiHgQOBs7rWlWSpGmlZ2Rk\npK2OEbE/8CbgRWB1Zv5LNwvbHcPDm9v7pSRJLxscnNPTrK2taaiIOBX43cx8IDMfBJbX2yRJM0C7\n01BLgPMbXl8EfB34d60GRcQQcCLVWseizHygoe30er/bgRWZubjZmIh4FfBV4PXAZuCczNzQZu2S\npN3U7gJ3T2Y+OvoiMx+nvjOqmYhYABydmScBFwJLx3RZCpwNnAycERHHtBhzETCcmW8H7gROabNu\nSdIEaPfK4smI+AJwP1XAvBtYUxhzGnA3QGY+HBEDEdGfmZsi4khgfWauAYiIFXX/wfHGAAuBz9Tb\nfb5DkiZZu2HxEeATwCVU00Mrgf9WGDMXWNXwerjetqn+PtzQtg44iuouq/HGzAPeExFfBJ4BLsnM\n9c0OPDDQR2/vrOIvJUlqT1thUd/59LmI6AGarpYXtBrXrK2n4Xtm5tUR8SngSuCKZjvbsGHLrlUo\nSTPY4OCcpm3t3g11RUT8BNgGbG343spaqquCUYcBTzdpO7ze1mzMs8B36m1/ARzbTt2SpInR7gL3\nBcBxmTmr/tonM0vzPPcB5wBExAnA2szcDC8vkPdHxLyI6AXOrPs3G/PnVOskAG+hem8qSdIkaXfN\n4pHMfLKTHWfmyohYFRErqe6cujQizgc2ZuYy4GLgG3X3OzNzNbB67Ji6fSnw1Yi4EHge+HAntUiS\ndk9bT3BHxDXA66juhto2uj0zb+taZbvBJ7glqXOtnuBu98riMKq3+TipYdsIMC3DQpI0sTp5b6h9\ngNdm5jPdLWn3eWUhSZ2biPeG+vfAj6imoYiIoYh434RUJ0ma9tq9G2oJ1fs1jd76eg1wVVcqkiRN\nO+2GxfOZ+ezoi8x8DnipOyVJkqabdhe4f1q/yV9PRAwAHwSm7edZSJImVrthcQnwZeBtwKPAd4GP\ndqsoSdL00m5YHJyZZ3a1EknStNXumsUNXa1CkjStdfJ5FvcDf0fDwnZmfrobRUmSppd2w+Kf6y9J\n0gzUyRPcBwGvy8y/j4h9MrPlx6pOJZ/glqTOTcQT3B+kmoL6g3rTzRFxwe6XJknaE7S7wP1fgTfz\nykehfgL4WFcqkiRNO+2GxcbMfPmzSjPzp/gEtyTNGO0ucD8XER8G9q8/we4/88pVhiRpL9fulcXH\nqZ7engN8Bdgf+LVuFSVJml5aXllExG9n5hLg1zPz1yepJknSNFOahrowIuYAH4yInxvb6EN5kjQz\nlKahzgNeqH/ePs6XJGkGKF1ZzM/Mz0VET2YunpSKJEnTTiksPlVPP/2XiHhqbGNm3tadsiRJ00kp\nLD4JvAc4EDhlTNsIYFhI0gzQ1ntDRcTZmXnXJNQzIXxvKEnqXKv3hirdOntlZl4LvD8izhrbnpm/\nOgH1SZKmudI01A/q7yupHsjbBqynmoKSJM0QpbD4m4hYBvxr4O+BA+qf7wN811lJmiFKz1lcBTwF\nHJ2Z/zEzzwDmAT8FrulybZKkaaJ0ZXEKcFpmbhvdkJlbIuISYBVwRavBETEEnEg1bbUoMx9oaDsd\nWEL1cN+K0ec4CmPeBXwrM5suwkiSJl7pymJbZu70VuSZuRX4SauBEbGA6orkJOBCYOmYLkuBs4GT\ngTMi4phWYyJiP+BK4OlCzZKkCVYKi1YL2dtatAGcBtwNkJkPAwMR0Q8QEUcC6zNzTf3xrCvq/k3H\nAL8NfAk/R0OSJl1pGurfRsST42zvAQ4ujJ1LNVU1arjetqn+3vh5GOuAo+p97jQmIuYCb87MT0fE\ndYXjMjDQR2/vrFI3SVKbSmERE3isVusMzdpGtw8Bv9nugTZs2FLuJEn6GYODc5q2tQyLzHxiN467\nluoKYtRhvLLeMLbt8HrbS+OMeRGYD/xhRAAcGhHfycwFu1GbJKkD7X6s6q64D7ga+L36o1jXZuZm\ngMx8PCL6I2Ie8GPgTOBcqmmosWOeoJqiAiAiHjcoJGlydS0sMnNlRKyKiJXADuDSiDgf2JiZy4CL\ngW/U3e/MzNXA6rFjulWfJKl9bb2R4J7GNxKUpM61eiPB0q2zkiQZFpKkMsNCklRkWEiSigwLSVKR\nYSFJKjIsJElFhoUkqciwkCQVGRaSpCLDQpJUZFhIkooMC0lSkWEhSSoyLCRJRYaFJKnIsJAkFRkW\nkqQiw0KSVGRYSJKKDAtJUpFhIUkqMiwkSUWGhSSpyLCQJBUZFpKkIsNCklRkWEiSinq7ufOIGAJO\nBEaARZn5QEPb6cASYDuwIjMXNxsTEUcAtwOvArYC52XmM92sXZL0iq5dWUTEAuDozDwJuBBYOqbL\nUuBs4GTgjIg4psWYzwG3ZOYCYBlwebfqliTtrJvTUKcBdwNk5sPAQET0A0TEkcD6zFyTmTuAFXX/\nZmMuAe6q9zsMHNTFuiVJY3RzGmousKrh9XC9bVP9fbihbR1wFHDweGMyczVARMwCLgU+2+rAAwN9\n9PbO2t36JUm1rq5ZjNGzC20vb6+D4g7grzPzr1odaMOGLZ1XJ0kz3ODgnKZt3QyLtVRXEKMOA55u\n0nZ4ve2lFmNuBx7JzKu7Uq0kqalurlncB5wDEBEnAGszczNAZj4O9EfEvIjoBc6s+487JiLOBV7K\nzM90sV5JUhM9IyMjXdt5RHweOBXYQbXWcDywMTOXRcSpwBfqrndl5vXjjcnMf4yIlcB+VOsdAP+U\nmZc0O+7w8Obu/VKStJcaHJzTdLmgq2ExVQwLSepcq7DwCW5JUpFhIUkqMiwkSUWGhSSpyLCQJBUZ\nFpKkIsNCklRkWEiSigwLSVKRYSFJKjIsJElFhoUkqciwkCQVGRaSpCLDQpJUZFhIkooMC0lSkWEh\nSSoyLCRJRYaFJKnIsJAkFRkWkqQiw0KSVGRYSJKKDAtJUpFhIUkq6p3qArTn2rRpI8uX38Ozzz7D\nIYfMZeHCs+jvP2Cqy5q2PF+d8Xx1ptvnq2dkZGTCdjZdDA9v3vt+qWlmaOg6brrpRrZseeHlbX19\ns1m06HIuu+yKKaxsevJ8dcbz1ZmJOl+Dg3N6mrV1NSwiYgg4ERgBFmXmAw1tpwNLgO3Aisxc3GxM\nRBwB3AHMAp4GPpSZLzY7rmHRXUND13HttYubtl955VX+H7qB56sznq/OTOT5mpKwiIgFwBWZeWZE\nvBG4LTNPamj/J+BdwFPAd4CPAYPjjYmI26kC5Y8jYgmwJjO/3OzYhkX3bNq0keOOm/8zf8GM1dc3\nm4ceSubM6Z/EyqYnz1dnPF+dmejz1SosurnAfRpwN0BmPgwMREQ/QEQcCazPzDWZuQNYUfdvNuYd\nwL31fpcDp3exbrWwfPk9Lf/DBNiy5QWWL79nkiqa3jxfnfF8dWYyz1c3F7jnAqsaXg/X2zbV34cb\n2tYBRwEHNxkzu2HaaR1waKsDDwz00ds7a7eK1/ief35D2/0GB+d0uZrpz/PVGc9XZybzfE3m3VBN\nL29atI23vdV+ANiwYUtbBalzr371QNv9hoc3d7ma6c/z1RnPV2cm+ny1CpRuTkOtpboqGHUY1eL0\neG2H19uajXk+IvYf01dTYOHCs+jrm92yT1/fbBYuPGuSKprePF+d8Xx1ZjLPVzfD4j7gHICIOAFY\nm5mbATLzcaA/IuZFRC9wZt2/2ZhvA2fX+z0b+FYX61YL/f0HsGjR5S37LFp0uYuPNc9XZzxfnZnM\n89XtW2c/D5wK7AAuBY4HNmbmsog4FfhC3fWuzLx+vDGZ+Y8RcSjwNWA/4AngI5m5tdlxvRuq+7wP\nvjOer854vjqzxz9nMVUMi8mxefOmnZ4Y9S++5jxfnfF8dWYizpdhIUkqmqrnLCRJewnDQpJUZFhI\nkor2yjULSdLE8spCklRkWEiSigwLSVKRYSFJKjIspElQvw/aSEScO2b741NTkdQZw0KaPKuBz0SE\nH8SgPY5hIU2ep4HbgaumuhCpU4aFNLluBN4XETHVhUidMCykSVR/PPAVwNKprkXqhGEhTbLMXAG8\nFBEfmOpapHZN5mdwS3rFbwF/Buw71YVI7fDKQpoCmfkj4Jv87GfOS9OWbyQoSSryykKSVGRYSJKK\nDAtJUpFhIUkqMiwkSUWGhSSpyLCQJBX9f+fzLUIijmHXAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}